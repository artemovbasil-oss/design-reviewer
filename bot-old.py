import os, io, json, re, statistics
from dataclasses import dataclass
from typing import List, Dict, Any, Tuple

from PIL import Image, ImageDraw
import pytesseract

from aiogram import Bot, Dispatcher, F
from aiogram.types import (
    Message,
    BufferedInputFile,
    ReplyKeyboardMarkup,
    KeyboardButton,
)
from aiogram.filters import CommandStart, Command
from aiogram.enums.parse_mode import ParseMode
from aiogram.client.default import DefaultBotProperties

from html import escape as htmlesc
from dotenv import load_dotenv

# --- –ù–∞–¥—ë–∂–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ .env (—Ä—è–¥–æ–º —Å bot.py) ---
BASE_DIR = os.path.abspath(os.path.dirname(__file__))
load_dotenv(os.path.join(BASE_DIR, ".env"))

BOT_TOKEN = os.getenv("BOT_TOKEN")
OCR_LANG = os.getenv("OCR_LANG", "rus+eng")
LLM_ENABLED = os.getenv("LLM_ENABLED", "false").lower() == "true"
LLM_MODEL = os.getenv("LLM_MODEL")
RULES_PATH = os.getenv("RULES_PATH", "rules.json")

# –ü–æ—Ä–æ–≥–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ OCR (–º–æ–∂–Ω–æ –ø—Ä–∞–≤–∏—Ç—å –≤ .env)
OCR_MIN_CONF = float(os.getenv("OCR_MIN_CONF", "55"))   # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ª–∏–Ω–∏–∏
OCR_MIN_WORD_CONF = float(os.getenv("OCR_MIN_WORD_CONF", "45"))  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Å–ª–æ–≤–∞
OCR_MIN_LEN = int(os.getenv("OCR_MIN_LEN", "2"))        # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
OCR_MIN_ALPHA_FRAC = float(os.getenv("OCR_MIN_ALPHA_FRAC", "0.45"))  # –¥–æ–ª—è –±—É–∫–≤/—Ü–∏—Ñ—Ä –≤ —Å—Ç—Ä–æ–∫–µ
BUTTON_MAX_WORDS = int(os.getenv("BUTTON_MAX_WORDS", "4"))           # —Å–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ –º–∞–∫—Å–∏–º—É–º –¥–ª—è –∫–Ω–æ–ø–∫–∏
HEADING_HEIGHT_MULT = float(os.getenv("HEADING_HEIGHT_MULT", "1.35"))# –ø–æ—Ä–æ–≥ –≤—ã—Å–æ—Ç—ã –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞

if not BOT_TOKEN:
    raise RuntimeError("Set BOT_TOKEN in .env or environment")

# --- aiogram setup ---
bot = Bot(BOT_TOKEN, default=DefaultBotProperties(parse_mode=ParseMode.HTML))
dp = Dispatcher()

# --- –î—Ä—É–∂–µ–ª—é–±–Ω–∞—è –∫–ª–∞–≤–∏–∞—Ç—É—Ä–∞ (–≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é) ---
main_kb = ReplyKeyboardMarkup(
    keyboard=[
        [KeyboardButton(text="üîç –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–µ–∫—Å—Ç"), KeyboardButton(text="üñº –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–∫—Ä–∏–Ω")],
        [KeyboardButton(text="üìò –ü—Ä–∞–≤–∏–ª–∞"), KeyboardButton(text="‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")],
    ],
    resize_keyboard=True,
    input_field_placeholder="–í—ã–±–µ—Ä–∏ –¥–µ–π—Å—Ç–≤–∏–µ üëá",
)

# --- LLM –º—è–≥–∫–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) ---
import llm_checker

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª ---
with open(RULES_PATH, "r", encoding="utf-8") as f:
    RULES_DB: Dict[str, Any] = json.load(f)

# ===================== –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê =====================

LATIN_TO_CYR = str.maketrans({
    "A": "–ê", "B": "–í", "C": "–°", "E": "–ï", "H": "–ù", "K": "–ö", "M": "–ú", "O": "–û", "P": "–†", "T": "–¢", "X": "–•", "Y": "–£",
    "a": "–∞", "c": "—Å", "e": "–µ", "o": "–æ", "p": "—Ä", "x": "—Ö", "y": "—É"
})

def normalize_text(s: str) -> str:
    if not s:
        return ""
    s = s.translate(LATIN_TO_CYR)
    s = s.replace("—ë", "–µ").replace("–Å", "–ï")
    s = s.replace("‚Äì", "-").replace("‚Äî", "-")
    s = s.replace("‚Äô", "'").replace("‚Äú", "\"").replace("‚Äù", "\"").replace("¬´", "\"").replace("¬ª", "\"")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def frac_alnum(s: str) -> float:
    if not s:
        return 0.0
    al = sum(ch.isalnum() for ch in s)
    return al / max(1, len(s))

# ===================== –î–û–ü. –†–ï–ì–£–õ–Ø–†–ö–ò-¬´–õ–û–í–£–®–ö–ò¬ª =====================

EXTRA_FORBIDDEN_REGEX = [
    r"\b–≤–Ω–∏–º–∞–Ω–∏[–µ—ë]\b\s*!*",
    r"\b–ø—Ä–æ–∏–∑–æ—à–ª–∞?\s+–æ—à–∏–±–∫[–∞–∏]\b\s*!*",
    r"\b–æ—à–∏–±–∫[–∞–∏]\b\s*!*",
    r"\b—É–≤–∞–∂–∞–µ–º\w*\b",
    r"\b–∫\s*—Å–æ–∂–∞–ª–µ–Ω\w*\b",
    r"\b—É—Å–ø–µ—à–Ω\w*\b",
    r"\b–æ—Ç–ø—Ä–∞–≤–ª–µ–Ω[–æ–∞]?\s+–≤\s+–æ–±—Ä–∞–±–æ—Ç–∫\w*\b",
    r"\b–æ–∂–∏–¥–∞–µ—Ç—Å—è\s+–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω\w*\b",
    r"\b–Ω–µ\s+–∏—Å–ø–æ–ª–Ω–µ–Ω\w*\b",
    r"\b–æ—à–∏–±–∫[–∞–∏]\s*\d{3,}\b",
    r"\binvalid\s+token\b",
]

# ===================== OCR (–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤ –º–∞—Å—à—Ç–∞–±–µ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞) =====================

def ocr_with_boxes(pil_img: Image.Image, lang: str = OCR_LANG):
    orig_w, orig_h = pil_img.size
    scale = 1.5 if max(orig_w, orig_h) < 1600 else 1.0
    work_img = pil_img if scale == 1.0 else pil_img.resize((int(orig_w * scale), int(orig_h * scale)), Image.LANCZOS)

    gray = work_img.convert("L")
    bw = gray.point(lambda x: 255 if x > 180 else 0, mode="1").convert("L")

    custom_config = "--oem 3 --psm 6"
    data = pytesseract.image_to_data(bw, lang=lang, config=custom_config, output_type=pytesseract.Output.DICT)

    inv = 1.0 / scale
    words = []
    for i in range(len(data["text"])):
        txt = (data["text"][i] or "").strip()
        if not txt:
            continue
        # —Ñ–∏–ª—å—Ç—Ä –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤–∞
        try:
            conf = float(data["conf"][i])
        except Exception:
            conf = -1.0
        if conf < OCR_MIN_WORD_CONF:
            continue

        x_s, y_s = int(data["left"][i]), int(data["top"][i])
        w_s, h_s = int(data["width"][i]), int(data["height"][i])

        x = int(x_s * inv); y = int(y_s * inv)
        w = max(1, int(w_s * inv)); h = max(1, int(h_s * inv))

        x = max(0, min(x, orig_w - 1)); y = max(0, min(y, orig_h - 1))
        if x + w > orig_w: w = orig_w - x
        if y + h > orig_h: h = orig_h - y

        words.append({"text": txt, "bbox": (x, y, w, h), "conf": conf})
    return words

def merge_words_to_lines(words, y_thresh=12):
    # –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ—Ç—Å–µ—á—ë–º —è–≤–Ω—ã–π –º—É—Å–æ—Ä –ø–æ —Å–∏–º–≤–æ–ª–∞–º
    clean = []
    for w in words:
        txt = normalize_text(w["text"])
        if len(txt) < 1:
            continue
        if frac_alnum(txt) < 0.3 and len(txt) < 3:
            continue
        clean.append({**w, "text": txt})

    words_sorted = sorted(clean, key=lambda w: (w["bbox"][1], w["bbox"][0]))
    lines, current, last_y = [], [], None
    for w in words_sorted:
        y = w["bbox"][1]
        if last_y is None or abs(y - last_y) <= y_thresh:
            current.append(w)
            if last_y is None:
                last_y = y
        else:
            lines.append(current)
            current = [w]
            last_y = y
    if current:
        lines.append(current)

    result = []
    for line in lines:
        text = " ".join([w["text"] for w in line]).strip()
        if not text:
            continue
        xs = [w["bbox"][0] for w in line]
        ys = [w["bbox"][1] for w in line]
        ws = [w["bbox"][2] for w in line]
        hs = [w["bbox"][3] for w in line]
        confs = [w["conf"] for w in line]

        x0, y0 = min(xs), min(ys)
        x1 = max(xs[i] + ws[i] for i in range(len(xs)))
        y1 = max(ys[i] + hs[i] for i in range(len(ys)))
        avg_conf = sum(confs) / max(1, len(confs))

        # —Ñ–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ª–∏–Ω–∏–∏
        if avg_conf < OCR_MIN_CONF:
            continue
        if len(text) < OCR_MIN_LEN:
            continue
        if frac_alnum(text.lower()) < OCR_MIN_ALPHA_FRAC:
            continue

        result.append({"text": text, "bbox": (x0, y0, x1 - x0, y1 - y0), "avg_conf": avg_conf, "height": (y1 - y0)})
    return result

# ===================== –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø –°–¢–†–û–ö =====================

ACTION_VERBS = {
    "–ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å","–æ—Ç–ø—Ä–∞–≤–∏—Ç—å","—É–¥–∞–ª–∏—Ç—å","–æ–ø–ª–∞—Ç–∏—Ç—å","—Å–æ–∑–¥–∞—Ç—å","–ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å","–∏–∑–º–µ–Ω–∏—Ç—å","–Ω–∞—á–∞—Ç—å",
    "–≤–µ—Ä–Ω—É—Ç—å—Å—è","–ø–æ–ª—É—á–∏—Ç—å","–ø–æ–≤—Ç–æ—Ä–∏—Ç—å","–∏—Å–ø—Ä–∞–≤–∏—Ç—å","—Å–∫–∞—á–∞—Ç—å","–æ—Ç–∫—Ä—ã—Ç—å","–ø–æ–ø–æ–ª–Ω–∏—Ç—å","–ø—Ä–æ–≤–µ—Ä–∏—Ç—å",
    "–ø–µ—Ä–µ–π—Ç–∏","–¥–æ–±–∞–≤–∏—Ç—å","–ø–æ–¥–ø–∏—Å–∞—Ç—å","—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å","–≤—ã–±—Ä–∞—Ç—å","–∑–∞–≤–µ—Ä—à–∏—Ç—å","–æ—Ñ–æ—Ä–º–∏—Ç—å","–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å","–ø–æ–¥–∫–ª—é—á–∏—Ç—å"
}

def classify_lines(lines: List[Dict[str, Any]]) -> None:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –ø–æ–ª–µ role: heading|button|text, —á—Ç–æ–±—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å –ø—Ä–∞–≤–∏–ª–∞."""
    if not lines:
        return
    heights = [ln["height"] for ln in lines]
    median_h = statistics.median(heights)

    for ln in lines:
        t = ln["text"]
        tl = t.lower()
        wc = len(t.split())
        h_ratio = (ln["height"] / max(1.0, median_h))

        looks_heading = (h_ratio >= HEADING_HEIGHT_MULT and wc >= 2) or (wc >= 4 and h_ratio >= 1.15)
        # –∫–Ω–æ–ø–∫–∞: –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç + –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –≥–ª–∞–≥–æ–ª–∞-–¥–µ–π—Å—Ç–≤–∏—è –ò–õ–ò –≤—Å—è —Å—Ç—Ä–æ–∫–∞ = 1-3 —Å–ª–æ–≤–∞ –±–µ–∑ —Ç–æ—á–∫–∏
        starts_with_action = any(tl.startswith(v) for v in ACTION_VERBS)
        looks_button = (wc <= BUTTON_MAX_WORDS and not t.endswith(".") and (starts_with_action or wc <= 3))

        if looks_button and not looks_heading:
            ln["role"] = "button"
        elif looks_heading:
            ln["role"] = "heading"
        else:
            ln["role"] = "text"

# ===================== –ü–†–ê–í–ò–õ–ê =====================

@dataclass
class Violation:
    rule_id: str
    title: str
    severity: str
    description: str
    suggestion: str
    text: str
    bbox: Tuple[int, int, int, int]
    kind: str  # "hard" | "soft"

def _match_any_regex(text: str, regex_list: List[str]) -> bool:
    return any(re.search(p, text, flags=re.IGNORECASE) for p in (regex_list or []))

def _match_any_exact(text: str, items: List[str]) -> bool:
    t = text.lower()
    return any((it or "").strip().lower() in t for it in (items or []))

def apply_rules(lines: List[Dict[str, Any]], rules: Dict[str, Any]) -> List[Violation]:
    violations: List[Violation] = []
    for line in lines:
        t_raw = line["text"]           # —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
        t = t_raw.lower()
        bbox = line["bbox"]
        role = line.get("role", "text")

        for r in rules.get("rules", []):
            rid, title, severity = r.get("id", ""), r.get("title", ""), r.get("severity", "low")
            desc, sugg = r.get("description", ""), r.get("suggestion", "")
            applies_to = r.get("applies_to")  # –º–æ–∂–µ—Ç –±—ã—Ç—å ["button"]

            # –ï—Å–ª–∏ –ø—Ä–∞–≤–∏–ª–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ —Ç–∏–ø–æ–º —ç–ª–µ–º–µ–Ω—Ç–∞ ‚Äî —É–≤–∞–∂–∞–µ–º —ç—Ç–æ
            if applies_to:
                if "button" in applies_to and role != "button":
                    continue

            # –ñ—ë—Å—Ç–∫–∏–µ –∑–∞–ø—Ä–µ—Ç—ã
            hard_hit = False
            if _match_any_exact(t, r.get("patterns_forbidden")):
                hard_hit = True
            if _match_any_regex(t, r.get("patterns_forbidden_regex")):
                hard_hit = True
            if not hard_hit and any(re.search(rx, t, flags=re.IGNORECASE) for rx in EXTRA_FORBIDDEN_REGEX):
                # –¥–æ–ø. –ª–æ–≤—É—à–∫–∏ –ø—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –∫ –æ–±—ã—á–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É/—Å—Ç–∞—Ç—É—Å–∞–º/–∑–∞–≥–æ–ª–æ–≤–∫–∞–º
                if role != "button":  # —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ—Ç—Ä–∏–≥–≥–µ—Ä–∏—Ç—å –∫–Ω–æ–ø–∫–∏ —Å–ª—É—á–∞–π–Ω–æ
                    hard_hit = True

            if hard_hit:
                violations.append(Violation(rid, title, severity, desc, sugg, t_raw, bbox, "hard"))
                continue

            # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—Ä–∞–≤–∏–ª–æ –∏—Ö —Ç—Ä–µ–±—É–µ—Ç)
            if r.get("patterns_required_any"):
                ok = any(k.strip().lower() in t for k in r["patterns_required_any"])
                if not ok:
                    violations.append(Violation(rid, title, severity, desc, sugg, t_raw, bbox, "hard"))
                    continue

            # –ú—è–≥–∫–∏–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏
            if r.get("soft_check") and _match_any_regex(t, r.get("patterns_forbidden_regex", [])):
                violations.append(Violation(rid, title, severity, desc, sugg, t_raw, bbox, "soft"))
    return violations

# ===================== –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø =====================

def draw_annotations(pil_img: Image.Image, violations: List[Violation]) -> Image.Image:
    img = pil_img.copy().convert("RGBA")
    draw = ImageDraw.Draw(img)
    for v in violations:
        x, y, w, h = v.bbox
        color = (255, 0, 0, 255) if v.kind == "hard" else (255, 165, 0, 255)
        draw.rectangle([x, y, x + w, y + h], outline=color, width=3)
        label = f"{v.rule_id}"
        tw = draw.textlength(label)
        pad = 4
        top = max(0, y - 18)
        draw.rectangle([x, top, x + int(tw) + 2*pad, top + 16], fill=color)
        draw.text((x + pad, top + 1), label, fill=(255, 255, 255, 255))
    return img

# ===================== –•–ï–ù–î–õ–ï–†–´ =====================

@dp.message(CommandStart())
async def start(m: Message):
    await m.answer(
        "üëã –ü—Ä–∏–≤–µ—Ç! –≠—Ç–æ Bereke UI Text Checker.\n"
        "‚Ä¢ –ü—Ä–æ–≤–µ—Ä—å —Ç–µ–∫—Å—Ç—ã –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–µ–¥–ø–æ–ª–∏—Ç–∏–∫–µ Bereke\n"
        "‚Ä¢ –û—Ç–ø—Ä–∞–≤—å —Å–∫—Ä–∏–Ω –¥–ª—è OCR-–ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π /check\n"
        "‚Ä¢ –î–ª—è –æ—Ç–ª–∞–¥–∫–∏ OCR ‚Äî /debug_ocr",
        reply_markup=main_kb,
    )

@dp.message(F.text == "üîç –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ç–µ–∫—Å—Ç")
async def shortcut_check(m: Message):
    await m.answer("–û—Ç–ø—Ä–∞–≤—å —Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π —Ö–æ—á–µ—à—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å:")

@dp.message(F.text == "üñº –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–∫—Ä–∏–Ω")
async def shortcut_image(m: Message):
    await m.answer("–ü—Ä–∏—à–ª–∏ —Å–∫—Ä–∏–Ω—à–æ—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ ‚Äî –Ω–∞–π–¥—É –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã üïµÔ∏è")

@dp.message(F.text == "üìò –ü—Ä–∞–≤–∏–ª–∞")
async def shortcut_rules(m: Message):
    await cmd_rules(m)

@dp.message(F.text == "‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
async def shortcut_settings(m: Message):
    await m.answer(
        "–ù–∞—Å—Ç—Ä–æ–π–∫–∏ OCR:\n"
        f"‚Ä¢ OCR_MIN_CONF: <code>{OCR_MIN_CONF}</code>\n"
        f"‚Ä¢ OCR_MIN_WORD_CONF: <code>{OCR_MIN_WORD_CONF}</code>\n"
        f"‚Ä¢ OCR_MIN_LEN: <code>{OCR_MIN_LEN}</code>\n"
        f"‚Ä¢ OCR_MIN_ALPHA_FRAC: <code>{OCR_MIN_ALPHA_FRAC}</code>\n"
        f"‚Ä¢ BUTTON_MAX_WORDS: <code>{BUTTON_MAX_WORDS}</code>\n"
        f"‚Ä¢ HEADING_HEIGHT_MULT: <code>{HEADING_HEIGHT_MULT}</code>\n"
        "\n–ò–∑–º–µ–Ω–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ .env –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏ –±–æ—Ç–∞.",
    )

@dp.message(Command("rules"))
async def cmd_rules(m: Message):
    meta = RULES_DB.get("meta", {})
    rules = RULES_DB.get("rules", [])
    head = f"<b>–ü—Ä–∞–≤–∏–ª–∞:</b> {len(rules)} —à—Ç. –ò—Å—Ç–æ—á–Ω–∏–∫: {htmlesc(meta.get('source', '?'))}\n"
    lines = []
    for r in rules[:30]:
        rid = htmlesc(r.get("id", ""))
        sev = htmlesc(r.get("severity", "low"))
        title = htmlesc(r.get("title", ""))
        lines.append(f"‚Ä¢ <b>{rid}</b> ({sev}): {title}")
    tail = f"\n‚Ä¶ –∏ –µ—â—ë {len(rules) - 30}" if len(rules) > 30 else ""
    await m.answer(head + "\n".join(lines) + tail)

@dp.message(Command("check"))
async def cmd_check(m: Message):
    payload = (m.text or "").partition(" ")[2].strip()
    if not payload:
        await m.answer("–§–æ—Ä–º–∞—Ç: <code>/check &lt;—Ç–µ–∫—Å—Ç&gt;</code>")
        return

    # –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ ‚Äî –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–∞—è "–ª–∏–Ω–∏—è"
    lines = [{"text": normalize_text(payload), "bbox": (0, 0, 100, 20), "height": 16}]
    classify_lines(lines)

    llm_issues_map = {}
    if LLM_ENABLED:
        try:
            llm_issues_map = llm_checker.llm_soft_checks([payload], model=LLM_MODEL)
        except Exception:
            llm_issues_map = {}

    violations = apply_rules(lines, RULES_DB)
    if LLM_ENABLED and llm_issues_map.get(0):
        for issue in llm_issues_map[0]:
            violations.append(
                Violation(
                    rule_id=str(issue.get("rule_id", "LLM")),
                    title="LLM-–º—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞",
                    severity=str(issue.get("severity", "low")).lower(),
                    description=str(issue.get("note", "")),
                    suggestion=str(issue.get("note", "")),
                    text=payload,
                    bbox=(0, 0, 100, 20),
                    kind="soft",
                )
            )

    if not violations:
        await m.answer("–ù–∞—Ä—É—à–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚úÖ")
        return

    chunks = ["<b>–ù–∞–π–¥–µ–Ω—ã –∑–∞–º–µ—á–∞–Ω–∏—è:</b>"]
    for v in violations[:50]:
        prefix = "üî¥" if v.kind == "hard" else "üü†"
        chunks.append(
            f"{prefix} <b>{htmlesc(v.rule_id)}</b> ({htmlesc(v.severity)}): "
            f"¬´{htmlesc(v.text)}¬ª ‚Äî {htmlesc(v.title)}. <i>{htmlesc(v.suggestion)}</i>"
        )
    await m.answer("\n".join(chunks))

@dp.message(Command("debug_ocr"))
async def cmd_debug_ocr(m: Message):
    await m.answer("–û–∫. –¢–µ–ø–µ—Ä—å –æ—Ç–≤–µ—Ç—å –Ω–∞ —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –ö–ê–†–¢–ò–ù–ö–û–ô ‚Äî –ø—Ä–∏—à–ª—é —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏, –∏—Ö —Ä–æ–ª—å (heading/button/text) –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é.")

@dp.message(F.photo | F.document[(F.document.mime_type.startswith("image/"))])
async def handle_image(m: Message):
    # –ü–æ–ª—É—á–∞–µ–º —Ñ–∞–π–ª
    if m.photo:
        file_id = m.photo[-1].file_id
    else:
        file_id = m.document.file_id
    file = await bot.get_file(file_id)
    f = await bot.download_file(file.file_path)
    img = Image.open(io.BytesIO(f.read())).convert("RGB")

    # OCR ‚Üí —Å–ª–æ–≤–∞ ‚Üí —Å—Ç—Ä–æ–∫–∏
    words = ocr_with_boxes(img, OCR_LANG)
    lines = merge_words_to_lines(words)
    classify_lines(lines)

    # debug-—Ä–µ–∂–∏–º
    is_debug_reply = bool(m.reply_to_message and "/debug_ocr" in (m.reply_to_message.text or ""))
    if is_debug_reply:
        debug_lines = []
        for ln in lines[:60]:
            role = ln.get("role", "text")
            debug_lines.append(
                f"‚Ä¢ ({role}) ¬´{htmlesc(ln['text'])}¬ª\n   norm: ¬´{htmlesc(normalize_text(ln['text']))}¬ª, h={ln.get('height')}, conf‚âà{int(ln.get('avg_conf',0))}"
            )
        if not debug_lines:
            await m.answer("–ù–∏—á–µ–≥–æ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ ü§∑‚Äç‚ôÇÔ∏è")
        else:
            chunk = []
            for i, row in enumerate(debug_lines, 1):
                chunk.append(row)
                if i % 25 == 0:
                    await m.answer("\n".join(chunk))
                    chunk = []
            if chunk:
                await m.answer("\n".join(chunk))
        return

    # LLM soft checks (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    llm_issues_map = {}
    if LLM_ENABLED:
        try:
            llm_issues_map = llm_checker.llm_soft_checks([ln["text"] for ln in lines], model=LLM_MODEL)
        except Exception:
            llm_issues_map = {}

    # –ü—Ä–∞–≤–∏–ª–∞
    violations = apply_rules(lines, RULES_DB)

    # LLM-–ø–æ–¥—Å–∫–∞–∑–∫–∏
    if LLM_ENABLED and llm_issues_map:
        for idx, items in llm_issues_map.items():
            if 0 <= idx < len(lines):
                line = lines[idx]
                for issue in items:
                    rid = str(issue.get("rule_id", "LLM"))
                    note = str(issue.get("note", ""))
                    sev = str(issue.get("severity", "low")).lower()
                    violations.append(
                        Violation(
                            rule_id=rid,
                            title="LLM-–º—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞",
                            severity=sev,
                            description=note or "",
                            suggestion=note or "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –ø–æ —Ä–µ–¥–ø–æ–ª–∏—Ç–∏–∫–µ.",
                            text=line["text"],
                            bbox=line["bbox"],
                            kind="soft",
                        )
                    )

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
    violations_sorted = sorted(
        violations,
        key=lambda v: (0 if v.kind == "hard" else 1, {"high": 0, "medium": 1, "low": 2}.get(v.severity, 2)),
    )

    # –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏
    annotated = draw_annotations(img, violations_sorted)
    buf = io.BytesIO()
    annotated.save(buf, format="PNG")
    buf.seek(0)

    # –û—Ç—á—ë—Ç
    if not violations_sorted:
        await m.answer("–ù–∞—Ä—É—à–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚úÖ")
    else:
        chunks = ["<b>–ù–∞–π–¥–µ–Ω—ã –∑–∞–º–µ—á–∞–Ω–∏—è:</b>"]
        for v in violations_sorted[:60]:
            prefix = "üî¥" if v.kind == "hard" else "üü†"
            chunks.append(
                f"{prefix} <b>{htmlesc(v.rule_id)}</b> ({htmlesc(v.severity)}): "
                f"¬´{htmlesc(v.text)}¬ª ‚Äî {htmlesc(v.title)}. "
                f"<i>{htmlesc(v.suggestion)}</i>"
            )
        text_out = "\n".join(chunks)
        while len(text_out) > 3500:
            cut = text_out[:3500]
            last_nl = cut.rfind("\n")
            if last_nl == -1:
                last_nl = 3500
            await m.answer(cut[:last_nl])
            text_out = text_out[last_nl+1:]
        if text_out:
            await m.answer(text_out)

    await m.answer_photo(BufferedInputFile(buf.getvalue(), filename="annotated.png"))

# ===================== –ó–ê–ü–£–°–ö =====================

if __name__ == "__main__":
    import asyncio
    print(f"‚úÖ Bereke bot starting... OCR_LANG={OCR_LANG}, LLM_ENABLED={LLM_ENABLED}, model={LLM_MODEL}, rules={RULES_PATH}")
    asyncio.run(dp.start_polling(bot))
